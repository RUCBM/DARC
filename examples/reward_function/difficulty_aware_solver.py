"""
Difficulty-aware solver reward function with selectable label source.

Two modes for gold label selection:
- rule: use dataset-provided gold from `reward_model = {"style": "rule", "ground_truth": str(answer)}`.
- self_vote: reuse the rollout n samples per prompt (already generated by trainer)
  to do majority voting as the pseudo-label, then score each sample against it.

Interface follows examples/reward_function/math.py:compute_score
and returns a list of dicts with keys {"overall", "format", "accuracy"}.

Usage in CLI:
  worker.reward.reward_function=./examples/reward_function/difficulty_aware_solver.py:compute_score
  (optionally) worker.reward.reward_function_kwargs.solver_label_mode=auto|rule|self_vote
  (optionally) worker.reward.reward_function_kwargs.format_weight=0.05

Environment overrides (optional):
- SOLVER_LABEL_MODE: same as solver_label_mode above, takes precedence if present.
- SOLVER_FORMAT_WEIGHT: float, overrides format_weight.

Notes:
- We do not call the actor again; for self_vote we group the rollout outputs by
  original sample using repeat_interleave semantics (n contiguous items per sample).
- This avoids modifying the trainer and ensures we use the current solver's outputs.
"""


from __future__ import annotations

import json
import os
import re
import ast
from collections import Counter
from typing import Any, Dict, List, Tuple

from mathruler.grader import extract_boxed_content, grade_answer


_BOXED_RE = re.compile(r"\\boxed\{(.*?)\}")


def _env_or_default(name: str, default: str) -> str:
    v = os.getenv(name)
    return v if v is not None and str(v).strip() != "" else default


def _to_canonical_json(obj: Any) -> str:

    try:
        return json.dumps(obj, sort_keys=True, ensure_ascii=False)
    except Exception:
        return str(obj)


def _parse_reward_model(item: Any) -> Dict[str, Any]:





    if isinstance(item, dict):
                                                                                       
                                                                                      
                                                                           
        if "style" not in item and "ground_truth" in item:
            extra_info = item.get("extra_info")
            parsed = _parse_reward_model(item.get("ground_truth"))
            if extra_info is not None and "extra_info" not in parsed:
                parsed = parsed.copy()
                parsed["extra_info"] = extra_info
            return parsed
                                                                      
        if "reward_model" in item and ("style" not in item or "ground_truth" not in item):
            extra_info = item.get("extra_info")
            parsed = _parse_reward_model(item.get("reward_model"))
            if extra_info is not None and "extra_info" not in parsed:
                parsed = parsed.copy()
                parsed["extra_info"] = extra_info
            return parsed
        return item
    if isinstance(item, str):
        s = item.strip()
        if s.startswith("{") and s.endswith("}"):
            try:
                return json.loads(s)
            except Exception:
                                                                            
                try:
                    obj = ast.literal_eval(s)
                    if isinstance(obj, dict):
                        return obj
                except Exception:
                    pass
                return {"style": "rule", "ground_truth": s}
                                                   
        return {"style": "rule", "ground_truth": s}
              
    return {"style": "rule", "ground_truth": str(item)}


def _extract_answer(text: str) -> str:


    try:
        ans = extract_boxed_content(text)
        if isinstance(ans, str) and ans.strip():
            return ans.strip()
    except Exception:
        pass

    m = None
    try:
        m = list(_BOXED_RE.finditer(text))
    except Exception:
        m = None
    if m:
        return m[-1].group(1).strip()
    return ""


def _format_score(predict: str, require_think: bool = False) -> float:





    if require_think:
                                                            
        pattern = re.compile(r"<think>.*?</think>.*?\\boxed\{.*?\}", re.DOTALL | re.IGNORECASE)
        return 1.0 if re.search(pattern, predict) else 0.0
                                                                              
    if not _BOXED_RE.search(predict):
        return 0.0
    outside = _BOXED_RE.sub("", predict)
    return 1.0 if outside.strip() else 0.0


def _majority_vote(answers: List[str]) -> Tuple[str, float]:

    if not answers:
        return "", 0.0
    cnt = Counter(a for a in answers if a != "")
    if not cnt:
        return "", 0.0
    most_common = cnt.most_common()
    top_count = most_common[0][1]
                                                            
    winners = {a for a, c in most_common if c == top_count}
    for a in answers:
        if a in winners:
            majority = a
            break
    frac = top_count / max(len(answers), 1)
    return majority, frac


def _group_indices_by_repeat(ground_truths: List[Any]) -> List[Tuple[int, int]]:






    groups: List[Tuple[int, int]] = []
    if not ground_truths:
        return groups
                                     
    keys = [_to_canonical_json(x) for x in ground_truths]
    start = 0
    for i in range(1, len(keys) + 1):
        if i == len(keys) or keys[i] != keys[start]:
            groups.append((start, i))
            start = i
    return groups


def _score_against_gold(pred: str, gold: str, format_weight: float, require_think: bool) -> Dict[str, float]:
    fmt = _format_score(pred, require_think=require_think)
    if fmt <= 0.0:
                                                               
        return {"overall": 0.0, "format": float(fmt), "accuracy": 0.0}
    pred_ans = _extract_answer(pred)
    acc = 0.0
    if pred_ans == "" or pred_ans.lower() == 'none':
        acc = 0.0
    else:
        try:
            acc = 1.0 if grade_answer(pred_ans, gold) else 0.0
        except Exception:
                                             
            acc = 1.0 if pred_ans != "" and pred_ans == str(gold).strip() else 0.0
    overall = (1.0 - format_weight) * acc + format_weight * fmt
    return {"overall": float(overall), "format": float(fmt), "accuracy": float(acc)}


def compute_score(
    predicts: List[str],
    ground_truths: List[Any],
    *,
    solver_label_mode: str = "auto",                                 
    format_weight: float = 0.05,
    require_think: bool = False,
                                                                                      
    label_prompt_key: str | None = None,
    label_n: int | None = None,
    label_temperature: float | None = None,
    label_top_p: float | None = None,
    label_top_k: int | None = None,
    **_: Any,
) -> List[Dict[str, float]]:









                   
    solver_label_mode = _env_or_default("SOLVER_LABEL_MODE", solver_label_mode).strip().lower()
    try:
        env_fw = _env_or_default("SOLVER_FORMAT_WEIGHT", str(format_weight))
        format_weight = float(env_fw)
    except Exception:
        pass

                                                                        
    parsed = [_parse_reward_model(gt) for gt in ground_truths]

                                   
    pred_answers = [_extract_answer(p) for p in predicts]

    results: List[Dict[str, float]] = []

    if solver_label_mode not in ("auto", "rule", "self_vote"):
        solver_label_mode = "auto"

    if solver_label_mode == "rule":
                                                                    
        for pred, info in zip(predicts, parsed):
            gold = str(info.get("ground_truth", ""))
            results.append(_score_against_gold(pred, gold, format_weight, require_think))
        return results

                                       
    groups = _group_indices_by_repeat(parsed)

                                                
    for (lo, hi) in groups:
                                                                 
        group_mode = solver_label_mode
        if group_mode == "auto":
            group_mode = "rule" if str(parsed[lo].get("style", "")).lower() == "rule" else "self_vote"
                                                                                
                                                                                      
        if group_mode == "self_vote":
                                                                                  
                              
                                     
            group_mode = "rule"

        if group_mode == "rule":
            gold = str(parsed[lo].get("ground_truth", ""))
            for i in range(lo, hi):
                results.append(_score_against_gold(predicts[i], gold, format_weight, require_think))
            continue

                   
        group_answers = [pred_answers[i] for i in range(lo, hi)]
        vote_answer, _ = _majority_vote(group_answers)
        for i in range(lo, hi):
            results.append(_score_against_gold(predicts[i], vote_answer, format_weight, require_think))

    return results
